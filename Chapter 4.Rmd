
# Clustering and Classification



## Introducing the Data


```{r, echo=FALSE, message=FALSE, warning=FALSE}

## Load The Data
library(MASS)
data("Boston")

## Explore the dataset
str(Boston)
##summary(Boston)

```


This week we are going to explore crime rates in different parts of Boston. Our dataset has `r dim(Boston)[[1]] ` observations and `r dim(Boston)[[2]] ` variables. In addition of crime rate, we know different attributes of the towns. You can find more information about the variables from [here][id3]

Variable  | Description
------------- | -------------
crim | per capita crime rate by town.
zn | proportion of residential land zoned for lots over 25,000 sq.ft.
indus | proportion of non-retail business acres per town.
chas | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox | nitrogen oxides concentration (parts per 10 million).
rm | average number of rooms per dwelling.
age | proportion of owner-occupied units built prior to 1940.
dis | weighted mean of distances to five Boston employment centres.
rad | index of accessibility to radial highways.
tax | full-value property-tax rate per \$10,000.
ptratio | pupil-teacher ratio by town.
black | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
lstat | lower status of the population (percent).
medv | median value of owner-occupied homes in \$1000s.



[id3]: https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html


## Graphical Overview


Before starting the analysis, let's have look of our data. Below we can see to plot matrices. From the first one, we can see the distributons of the variables in our dataset. The second one illustrates the correlations between the variables. The bigger the circle, the bigger the correlations of those variables. The color of the circles tells us the whether the correlation is negative (red) or positive (blue). 


```{r, echo=FALSE, message=FALSE, warning=FALSE}

## Create a plot matrix using ggpairs()
library(GGally) ; library(ggplot2)
p <- ggpairs(Boston, lower = "blank",  
             upper = list(continuous = "points", combo =
                                 "facethist", discrete = "facetbar", na = "na"))
p

```


```{r, echo=FALSE, message=FALSE, warning=FALSE}

## Calculate the correlation matrix and visualize it
library(corrplot) ; library(dplyr)
cor_matrix <- cor(Boston) %>% round(digits = 2)
corrplot(cor_matrix, method="circle", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)

```


## Standardize the Dataset


After having standardized the dataset we can instantly see that the mean of every variable is equal to zero. In addition, we can observe that the variances of the variables are all equal to one.  


```{r, echo=FALSE, message=FALSE, warning=FALSE}

## Standardize and change it as a data frame
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)

## Summaries of the scaled variables
summary(boston_scaled)
library(dplyr)
boston_scaled %>% var()%>% round(digits = 2)

```


Now we will create a new categorical variable for crime rate and divide the dataset into two parts; one for trainng and one for testing. 


```{r, echo=FALSE, message=FALSE, warning=FALSE}

## Create a quantile vector of crim and use it to create a categorical variable 'crime'
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high") )
boston_scaled <- data.frame(boston_scaled, crime)

## Remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

## Divide the dataset to train and test sets
ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
```


## Linear Discriminant Analysis

Next, we will fit the linear discriminant analysis using our train set. Our dependant variable is crime rate and rest of the variables are used as predictors. The plot below illustrates the results. We can see that high crime rate is clearly separeted from the rest and it is the variable *rad* that predicts this separation. From low to medium high crime rate, it seems that variables *zn* and *nox* are the biggest determinants.  


```{r, echo=FALSE, message=FALSE, warning=FALSE}

## Fit the LDA
lda.fit <- lda(crime ~ . , data = train)

## Function for LDA biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

## target classes as numeric
classes <- as.numeric(train$crime)

## Plot the LDA
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```



## Predictions

Now, we will examine the predictive power of this analysis. If our model had categorised every observation correctly, the table below would have only zeros execpt for the diagonal. We can see that this is not the case, even though most of the observations are correctly predicted. In addition, we can see that our model succeeded better in categorising the high crime rates as it did for the low.

```{r, echo=FALSE, message=FALSE, warning=FALSE}

## Save the correct classes from test data and remove the crime variable from test data
correct_classes <- test$crime
test <- dplyr::select(test, -crime)

## Predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

## Cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```


## K-means

Lastly, we will perform the k-means algorithm on our dataset. We will start this by reloding the data, scaling it and calulating the distances between the variables. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

## Reload the Boston dataset
library(MASS)
data('Boston')

## Standardize an change it as a data frame
boston_scaled <- scale(Boston)
boston_scaled <-  as.data.frame(boston_scaled)

## Calculate the euclidean and manhattan distance matrixes
dist_eu <- dist(boston_scaled)
dist_man <- dist(boston_scaled, "manhattan")

## look at the summary of the distances
summary(dist_eu)
summary(dist_man)

```


Now, let's run the k-means algorithm with 4 clusters.  We can decide the optimal number of cluters from the plot below. The opitmal number is found at the point when the within cluster sum of squares (WCSS) drops radically. In this case, this point is when there are two clusters. 


```{r, echo=FALSE, message=FALSE, warning=FALSE}

## K-means clustering with k=4
km <- kmeans(boston_scaled , centers = 4)

## Find out the optimal number of clusters
library(ggplot2)
set.seed(123)
twcss <- sapply(1:10, function(k){kmeans(boston_scaled , k)$tot.withinss})
qplot(x = 1:10, y = twcss, geom = 'line')

```

Let's run the algorithm again and plot it. We can see that the k-means with two clusters works quite nicely. We can see that there are usually two different distributions in in each variable. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}

## K-means clustering with k=2
km <-kmeans(boston_scaled , centers = 2)

## Plot the Boston dataset with clusters
p <- ggpairs(boston_scaled, lower = "blank",  
             upper = list(continuous = "points", combo =
                            "facethist", discrete = "facetbar", na = "na"),
             mapping = aes(col = as.factor(km$cluster), alpha = 0.3))
p


```



## Super Bonus

Interactive plot:
```{r}

library(plotly)
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3,
        type= 'scatter3d', mode='markers', color = train$crime)

```


```{r}

## Another plot
km2 <- kmeans(boston_scaled[as.numeric(row.names(train)),], centers = 2)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3,
        type= 'scatter3d', mode='markers', color = km2$cluster)


```

